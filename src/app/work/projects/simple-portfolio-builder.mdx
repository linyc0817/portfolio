---
title: "MUILab Graduate-Related FAQ Chatbot"
publishedAt: "2025-10-21"
images:
  - "/images/projects/project-04/rag-01.jpeg"
  - "/images/projects/project-04/rag-02.jpeg"
  - "/images/projects/project-04/rag-demo.mp4"
---

## Overview
Developed a Retrieval-Augmented Generation (RAG) chatbot to answer graduate-related FAQ using both local document retrieval and LLM-based reasoning.

## Key Features
- **Dual-Mode Answering Logic**: Designed a decision flow where the chatbot dynamically switches between RAG mode and general GPT mode based on document similarity thresholds, 
ensuring accurate yet efficient responses.
- **Retrieval-Augmented Generation Pipeline**: Implemented document parsing, chunking (size = 1000 / overlap = 200), embedding via OpenAI Embeddings, 
and vector storage using ChromaDB for persistent retrieval.
- **Performance–Accuracy Trade-off**: Tuned parameters such as Top-k = 5 and model temperature per mode (RAG = 0.3 / General = 0.7) to balance response precision, readability, and speed.
- **Error Handling and Fallback Strategy**: Built robust exception handling for encoding errors and empty retrievals; gracefully reverted to general GPT answers with explicit source labeling when documents were insufficient.

## Technologies Used
- **Core Stack**: Python, LangChain, ChromaDB, OpenAI API
- **Embeddings & Vectorization**: OpenAI Embeddings, Chroma Persistent Collections
- **Infrastructure**: Docker / Docker Compose for modular deployment

## Outcome

- Delivered a fully functional RAG chatbot capable of answering lab-specific FAQ from uploaded materials (PDF, DOCX, TXT).

## System Flow

- The chatbot compares query–document similarity and switches to RAG mode only if relevant chunks (distance < 0.5) are found.

<img
  src="/images/projects/project-04/system-flow.png"
  alt="RAG chatbot flow diagram"
  className="mx-auto max-h-[900px] w-auto object-contain"
/>




